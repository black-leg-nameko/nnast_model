#!/usr/bin/env python3
"""
Create GitHub issues for vulnerabilities detected by GNN model.

This script processes GNN inference results and creates GitHub issues
with LLM-generated fix suggestions.
"""
import argparse
import json
import os
import sys
import pathlib
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlparse

from data.github_api import GitHubAPIClient, load_env_file
from ml.code_fixer import LLMCodeFixer, FixSuggestion


def parse_repo_url(repo_url: str) -> Optional[tuple[str, str]]:
    """
    Parse repository URL to extract owner and repo name.
    
    Args:
        repo_url: Repository URL (e.g., "https://github.com/owner/repo")
        
    Returns:
        Tuple of (owner, repo) or None
    """
    try:
        parsed = urlparse(repo_url)
        if parsed.netloc in ["github.com", "www.github.com"]:
            parts = parsed.path.strip("/").split("/")
            if len(parts) >= 2:
                return (parts[0], parts[1])
    except Exception as e:
        print(f"Error parsing repo URL: {e}")
    
    return None


def load_inference_results(results_file: Path) -> List[Dict]:
    """
    Load GNN inference results from JSON file.
    
    Expected format:
    [
        {
            "file_path": "path/to/file.py",
            "repo_url": "https://github.com/owner/repo",
            "is_vulnerable": true,
            "confidence": 0.95,
            "vulnerability_type": "SQL Injection" (optional),
            "code": "vulnerable code content" (optional)
        },
        ...
    ]
    
    Args:
        results_file: Path to inference results JSON file
        
    Returns:
        List of inference result dictionaries
    """
    if not results_file.exists():
        print(f"Error: Results file not found: {results_file}")
        return []
    
    try:
        with open(results_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Handle both list and single object
        if isinstance(data, list):
            return data
        elif isinstance(data, dict):
            return [data]
        else:
            print(f"Error: Invalid JSON format in {results_file}")
            return []
            
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON: {e}")
        return []
    except Exception as e:
        print(f"Error reading file: {e}")
        return []


def get_file_content(
    repo_url: str,
    file_path: str,
    github_client: GitHubAPIClient
) -> Optional[str]:
    """
    Get file content from repository.
    
    Args:
        repo_url: Repository URL
        file_path: Path to file in repository
        github_client: GitHub API client
        
    Returns:
        File content as string, or None
    """
    repo_info = parse_repo_url(repo_url)
    if not repo_info:
        print(f"Error: Could not parse repo URL: {repo_url}")
        return None
    
    owner, repo = repo_info
    return github_client.get_file_content(owner, repo, file_path)


def create_issue_body(
    file_path: str,
    fix_suggestion: FixSuggestion,
    original_code: Optional[str] = None,
    confidence: Optional[float] = None
) -> str:
    """
    Create GitHub issue body with fix suggestion.
    
    Args:
        file_path: Path to vulnerable file
        fix_suggestion: LLM-generated fix suggestion
        original_code: Original vulnerable code (optional)
        confidence: Detection confidence (optional)
        
    Returns:
        Markdown-formatted issue body
    """
    body = f"""## Security Vulnerability Detected

**File**: `{file_path}`

"""
    
    if fix_suggestion.vulnerability_type:
        body += f"**Vulnerability Type**: {fix_suggestion.vulnerability_type}\n\n"
    
    if confidence is not None:
        body += f"**Detection Confidence**: {confidence:.1%}\n\n"
    
    body += "---\n\n"
    body += "## Explanation\n\n"
    body += f"{fix_suggestion.explanation}\n\n"
    body += "---\n\n"
    
    if original_code:
        body += "## Vulnerable Code\n\n"
        body += f"```python\n{original_code}\n```\n\n"
        body += "---\n\n"
    
    body += "## Suggested Fix\n\n"
    body += f"```python\n{fix_suggestion.fixed_code}\n```\n\n"
    body += "---\n\n"
    body += "## Notes\n\n"
    body += "- This issue was automatically generated by NNAST vulnerability detection system.\n"
    body += "- Please review the suggested fix carefully before applying.\n"
    body += "- Consider additional security testing and code review.\n"
    
    return body


def save_markdown_report(
    file_path: str,
    fix_suggestion: FixSuggestion,
    original_code: Optional[str] = None,
    confidence: Optional[float] = None,
    repo_url: Optional[str] = None,
    output_path: Path = None
) -> bool:
    """
    Save vulnerability report as Markdown file.
    
    Args:
        file_path: Path to vulnerable file
        fix_suggestion: LLM-generated fix suggestion
        original_code: Original vulnerable code (optional)
        confidence: Detection confidence (optional)
        repo_url: Repository URL (optional)
        output_path: Path to save Markdown file
        
    Returns:
        True if saved successfully, False otherwise
    """
    if output_path is None:
        return False
    
    # Create safe filename from file path
    safe_filename = file_path.replace("/", "_").replace("\\", "_").replace(" ", "_")
    if len(safe_filename) > 200:
        safe_filename = safe_filename[:200]
    
    # Add timestamp to avoid conflicts
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    markdown_file = output_path / f"vulnerability_{timestamp}_{safe_filename}.md"
    
    # Create markdown content
    content = f"""# Security Vulnerability Report

**Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## File Information

**File Path**: `{file_path}`

"""
    
    if repo_url:
        content += f"**Repository**: {repo_url}\n\n"
    
    if fix_suggestion.vulnerability_type:
        content += f"**Vulnerability Type**: {fix_suggestion.vulnerability_type}\n\n"
    
    if confidence is not None:
        content += f"**Detection Confidence**: {confidence:.1%}\n\n"
    
    content += "---\n\n"
    content += "## Explanation\n\n"
    content += f"{fix_suggestion.explanation}\n\n"
    content += "---\n\n"
    
    if original_code:
        content += "## Vulnerable Code\n\n"
        content += f"```python\n{original_code}\n```\n\n"
        content += "---\n\n"
    
    content += "## Suggested Fix\n\n"
    content += f"```python\n{fix_suggestion.fixed_code}\n```\n\n"
    content += "---\n\n"
    content += "## Notes\n\n"
    content += "- This report was automatically generated by NNAST vulnerability detection system.\n"
    content += "- Please review the suggested fix carefully before applying.\n"
    content += "- Consider additional security testing and code review.\n"
    
    try:
        with open(markdown_file, "w", encoding="utf-8") as f:
            f.write(content)
        return True
    except Exception as e:
        print(f"  Error saving markdown file: {e}")
        return False


def process_inference_results(
    results_file: Path,
    code_fixer: LLMCodeFixer,
    output_dir: Path,
    github_client: Optional[GitHubAPIClient] = None,
    min_confidence: float = 0.7
) -> Dict[str, int]:
    """
    Process inference results and save Markdown reports.
    
    Args:
        results_file: Path to inference results JSON file
        code_fixer: LLM code fixer
        output_dir: Directory to save Markdown reports
        github_client: Optional GitHub API client (for fetching file content)
        min_confidence: Minimum confidence threshold for creating reports
        
    Returns:
        Dictionary with statistics (created, skipped, errors)
    """
    results = load_inference_results(results_file)
    
    if not results:
        print("No inference results found")
        return {"created": 0, "skipped": 0, "errors": 0}
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    stats = {"created": 0, "skipped": 0, "errors": 0}
    
    print(f"Processing {len(results)} inference results...")
    print(f"Output directory: {output_dir}")
    
    for i, result in enumerate(results, 1):
        file_path = result.get("file_path")
        repo_url = result.get("repo_url", "")
        is_vulnerable = result.get("is_vulnerable", False)
        confidence = result.get("confidence", 1.0)
        
        print(f"\n[{i}/{len(results)}] Processing: {file_path}")
        
        # Skip if not vulnerable
        if not is_vulnerable:
            print(f"  Skipping: Not detected as vulnerable")
            stats["skipped"] += 1
            continue
        
        # Skip if confidence too low
        if confidence < min_confidence:
            print(f"  Skipping: Confidence {confidence:.1%} below threshold {min_confidence:.1%}")
            stats["skipped"] += 1
            continue
        
        # Get file content if not provided
        code = result.get("code")
        if not code and github_client and repo_url:
            print(f"  Fetching file content from GitHub...")
            code = get_file_content(repo_url, file_path, github_client)
            if not code:
                print(f"  Warning: Could not fetch file content, proceeding without code")
        elif not code:
            print(f"  Warning: No code content available, proceeding without code")
        
        # Generate fix suggestion
        print(f"  Generating fix suggestion...")
        vulnerability_type = result.get("vulnerability_type")
        fix_suggestion = code_fixer.generate_fix(
            vulnerable_code=code or "# Code content not available",
            file_path=file_path,
            vulnerability_type=vulnerability_type
        )
        
        if not fix_suggestion:
            print(f"  Error: Failed to generate fix suggestion")
            stats["errors"] += 1
            continue
        
        # Save markdown report
        print(f"  Saving Markdown report...")
        success = save_markdown_report(
            file_path=file_path,
            fix_suggestion=fix_suggestion,
            original_code=code,
            confidence=confidence,
            repo_url=repo_url,
            output_path=output_dir
        )
        
        if success:
            print(f"  ✓ Markdown report saved")
            stats["created"] += 1
        else:
            print(f"  ✗ Failed to save Markdown report")
            stats["errors"] += 1
        
        # Rate limiting delay for LLM API
        import time
        time.sleep(1)
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="Generate Markdown reports for vulnerabilities detected by GNN model"
    )
    parser.add_argument(
        "results_file",
        type=Path,
        help="Path to GNN inference results JSON file"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("vulnerability_reports"),
        help="Directory to save Markdown reports (default: vulnerability_reports)"
    )
    parser.add_argument(
        "--min-confidence",
        type=float,
        default=0.7,
        help="Minimum confidence threshold for creating reports (default: 0.7)"
    )
    parser.add_argument(
        "--llm-provider",
        choices=["openai", "anthropic"],
        default="openai",
        help="LLM provider for code fixing (default: openai)"
    )
    parser.add_argument(
        "--llm-model",
        default="gpt-4o",
        help="LLM model name (default: gpt-4o)"
    )
    parser.add_argument(
        "--fetch-code",
        action="store_true",
        help="Fetch code content from GitHub (requires GITHUB_TOKEN)"
    )
    
    args = parser.parse_args()
    
    # Load environment variables from .env file
    load_env_file()
    
    # Check API keys
    if args.llm_provider == "openai":
        if not os.getenv("OPENAI_API_KEY"):
            print("Error: OPENAI_API_KEY not set")
            return 1
    elif args.llm_provider == "anthropic":
        if not os.getenv("ANTHROPIC_API_KEY"):
            print("Error: ANTHROPIC_API_KEY not set")
            return 1
    
    # Initialize clients
    github_client = None
    if args.fetch_code:
        if not os.getenv("GITHUB_TOKEN"):
            print("Warning: GITHUB_TOKEN not set. Cannot fetch code from GitHub.")
            print("Reports will be generated without code content if not available in results.")
        else:
            github_client = GitHubAPIClient()
    
    code_fixer = LLMCodeFixer(provider=args.llm_provider, model=args.llm_model)
    
    # Process results
    stats = process_inference_results(
        results_file=args.results_file,
        code_fixer=code_fixer,
        output_dir=args.output_dir,
        github_client=github_client,
        min_confidence=args.min_confidence
    )
    
    # Print summary
    print("\n" + "="*50)
    print("Summary:")
    print(f"  Reports created: {stats['created']}")
    print(f"  Skipped: {stats['skipped']}")
    print(f"  Errors: {stats['errors']}")
    print(f"  Output directory: {args.output_dir}")
    print("="*50)
    
    return 0 if stats["errors"] == 0 else 1


if __name__ == "__main__":
    sys.exit(main())

