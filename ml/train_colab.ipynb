{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NNAST Model Training on Google Colab\n",
        "\n",
        "This notebook trains the NNAST model using GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPUç¢ºèª\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¾ãŸã¯ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# æ–¹æ³•1: Gitãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆæ¨å¥¨ï¼‰\n",
        "# !git clone https://github.com/yourusername/nnast_model.git\n",
        "# os.chdir('nnast_model')\n",
        "\n",
        "# æ–¹æ³•2: zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.zip'):\n",
        "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#             zip_ref.extractall('.')\n",
        "#         print(f\"Extracted {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-geometric\n",
        "!pip install transformers\n",
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ–¹æ³•1: Google Driveã‹ã‚‰ãƒã‚¦ãƒ³ãƒˆï¼ˆæ¨å¥¨ï¼‰\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ã€ã“ã“ã§ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "# ä¾‹: /content/drive/MyDrive/nnast_dataset/training_data\n",
        "DATASET_BASE = \"/content/drive/MyDrive/nnast_dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ–¹æ³•2: ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# \n",
        "# import zipfile\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.zip'):\n",
        "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#             zip_ref.extractall('.')\n",
        "#         print(f\"Extracted {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ã®ç¢ºèªã¨è‡ªå‹•æ¤œå‡º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Driveå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡º\n",
        "import glob\n",
        "\n",
        "def find_dataset_directory():\n",
        "    \"\"\"Google Driveå†…ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢\"\"\"\n",
        "    drive_base = \"/content/drive/MyDrive\"\n",
        "    \n",
        "    if not os.path.exists(drive_base):\n",
        "        print(\"âš ï¸ Google DriveãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Cell 6ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "        return None, None\n",
        "    \n",
        "    # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆé †ä½é †ï¼‰\n",
        "    patterns = [\n",
        "        \"**/nnast_colab_dataset/training_data/train_graphs.jsonl\",\n",
        "        \"**/nnast_dataset/training_data/train_graphs.jsonl\",\n",
        "        \"**/training_data/train_graphs.jsonl\",\n",
        "        \"**/train_graphs.jsonl\",\n",
        "    ]\n",
        "    \n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...\")\n",
        "    for pattern in patterns:\n",
        "        full_pattern = os.path.join(drive_base, pattern)\n",
        "        matches = glob.glob(full_pattern, recursive=True)\n",
        "        if matches:\n",
        "            graphs_file = matches[0]\n",
        "            print(f\"âœ“ è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {graphs_file}\")\n",
        "            # training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—\n",
        "            dataset_dir = os.path.dirname(graphs_file)\n",
        "            base_dir = os.path.dirname(dataset_dir)\n",
        "            return base_dir, graphs_file\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "# DATASET_BASEãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
        "if 'DATASET_BASE' not in globals():\n",
        "    DATASET_BASE = \"/content/drive/MyDrive/nnast_dataset\"\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢\n",
        "found_base, found_graphs = find_dataset_directory()\n",
        "\n",
        "if found_base:\n",
        "    print(f\"\\nâœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {found_base}\")\n",
        "    DATASET_BASE = found_base\n",
        "    print(f\"  ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ›´æ–°: {DATASET_BASE}\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ è‡ªå‹•æ¤œå‡ºã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "    print(f\"  ç¾åœ¨ã®è¨­å®š: {DATASET_BASE}\")\n",
        "    print(f\"\\nGoogle Driveå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§:\")\n",
        "    \n",
        "    # Google Driveå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’è¡¨ç¤º\n",
        "    drive_base = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(drive_base):\n",
        "        items = os.listdir(drive_base)\n",
        "        if items:\n",
        "            for item in items[:20]:  # æœ€åˆã®20å€‹ã®ã¿è¡¨ç¤º\n",
        "                item_path = os.path.join(drive_base, item)\n",
        "                if os.path.isdir(item_path):\n",
        "                    print(f\"  ğŸ“ {item}/\")\n",
        "                    # training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹ã‹ç¢ºèª\n",
        "                    training_data_path = os.path.join(item_path, \"training_data\")\n",
        "                    if os.path.exists(training_data_path):\n",
        "                        print(f\"     â””â”€ training_data/ (è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼)\")\n",
        "                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª\n",
        "                        graphs_path = os.path.join(training_data_path, \"train_graphs.jsonl\")\n",
        "                        labels_path = os.path.join(training_data_path, \"train_labels.jsonl\")\n",
        "                        if os.path.exists(graphs_path):\n",
        "                            print(f\"        âœ“ train_graphs.jsonl\")\n",
        "                        if os.path.exists(labels_path):\n",
        "                            print(f\"        âœ“ train_labels.jsonl\")\n",
        "        else:\n",
        "            print(\"  (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç©ºã§ã™)\")\n",
        "    else:\n",
        "        print(\"  Google DriveãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š\n",
        "GRAPHS_FILE = os.path.join(DATASET_BASE, \"training_data\", \"train_graphs.jsonl\")\n",
        "LABELS_FILE = os.path.join(DATASET_BASE, \"training_data\", \"train_labels.jsonl\")\n",
        "\n",
        "# è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹ã‚’ä½¿ç”¨\n",
        "if found_graphs:\n",
        "    GRAPHS_FILE = found_graphs\n",
        "    LABELS_FILE = os.path.join(os.path.dirname(found_graphs), \"train_labels.jsonl\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ™ãƒ¼ã‚¹: {DATASET_BASE}\")\n",
        "print(f\"ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«: {GRAPHS_FILE}\")\n",
        "print(f\"ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {LABELS_FILE}\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n",
        "files_ok = True\n",
        "\n",
        "if os.path.exists(GRAPHS_FILE):\n",
        "    file_size = os.path.getsize(GRAPHS_FILE) / (1024 * 1024)  # MB\n",
        "    print(f\"âœ“ train_graphs.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ ({file_size:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"âœ— train_graphs.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "    print(f\"  ãƒ‘ã‚¹: {GRAPHS_FILE}\")\n",
        "    files_ok = False\n",
        "\n",
        "if os.path.exists(LABELS_FILE):\n",
        "    file_size = os.path.getsize(LABELS_FILE) / (1024 * 1024)  # MB\n",
        "    print(f\"âœ“ train_labels.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ ({file_size:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"âœ— train_labels.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "    print(f\"  ãƒ‘ã‚¹: {LABELS_FILE}\")\n",
        "    files_ok = False\n",
        "\n",
        "print()\n",
        "\n",
        "# ä¸¡æ–¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å­¦ç¿’ã‚’ç¶šè¡Œ\n",
        "if files_ok:\n",
        "    print(\"âœ“ ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæº–å‚™ã§ãã¾ã—ãŸã€‚Cell 11ã§å­¦ç¿’ã‚’é–‹å§‹ã§ãã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\")\n",
        "    print(\"\\nã€æ–¹æ³•1ã€‘æ‰‹å‹•ã§ãƒ‘ã‚¹ã‚’è¨­å®š:\")\n",
        "    print(\"  ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€å®Ÿéš›ã®ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„:\")\n",
        "    print('  GRAPHS_FILE = \"/content/drive/MyDrive/å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/training_data/train_graphs.jsonl\"')\n",
        "    print('  LABELS_FILE = \"/content/drive/MyDrive/å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/training_data/train_labels.jsonl\"')\n",
        "    print(\"\\nã€æ–¹æ³•2ã€‘ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢:\")\n",
        "    print(\"  æ¬¡ã®ã‚»ãƒ«ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã§ãã¾ã™:\")\n",
        "    print('  !find /content/drive/MyDrive -name \"train_graphs.jsonl\" 2>/dev/null')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒãƒƒã‚°: Google Driveå†…ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè¡Œï¼‰\n",
        "# ã“ã®ã‚»ãƒ«ã¯ã€Cell 9ã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
        "\n",
        "print(\"Google Driveå†…ã§train_graphs.jsonlã‚’æ¤œç´¢ä¸­...\")\n",
        "!find /content/drive/MyDrive -name \"train_graphs.jsonl\" -type f 2>/dev/null | head -10\n",
        "\n",
        "print(\"\\nGoogle Driveå†…ã§train_labels.jsonlã‚’æ¤œç´¢ä¸­...\")\n",
        "!find /content/drive/MyDrive -name \"train_labels.jsonl\" -type f 2>/dev/null | head -10\n",
        "\n",
        "print(\"\\nã‚‚ã—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã—ã¦ãã ã•ã„:\")\n",
        "print(\"GRAPHS_FILE = \\\"è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹/train_graphs.jsonl\\\"\")\n",
        "print(\"LABELS_FILE = \\\"è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹/train_labels.jsonl\\\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ‰‹å‹•ã§ãƒ‘ã‚¹ã‚’è¨­å®šã™ã‚‹å ´åˆï¼ˆCell 9ã§è‡ªå‹•æ¤œå‡ºã§ããªã‹ã£ãŸå ´åˆï¼‰\n",
        "# ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’å®Ÿéš›ã®Google Driveå†…ã®ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„\n",
        "\n",
        "# ä¾‹: GRAPHS_FILE = \"/content/drive/MyDrive/nnast_colab_dataset/training_data/train_graphs.jsonl\"\n",
        "# ä¾‹: LABELS_FILE = \"/content/drive/MyDrive/nnast_colab_dataset/training_data/train_labels.jsonl\"\n",
        "\n",
        "# ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ã€å®Ÿéš›ã®ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„:\n",
        "# GRAPHS_FILE = \"/content/drive/MyDrive/å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/training_data/train_graphs.jsonl\"\n",
        "# LABELS_FILE = \"/content/drive/MyDrive/å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/training_data/train_labels.jsonl\"\n",
        "\n",
        "# è¨­å®šå¾Œã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª\n",
        "# if 'GRAPHS_FILE' in locals() and os.path.exists(GRAPHS_FILE):\n",
        "#     print(f\"âœ“ GRAPHS_FILE ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ: {GRAPHS_FILE}\")\n",
        "# if 'LABELS_FILE' in locals() and os.path.exists(LABELS_FILE):\n",
        "#     print(f\"âœ“ LABELS_FILE ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ: {LABELS_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. å­¦ç¿’ã®å®Ÿè¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "OUTPUT_DIR = \"./checkpoints\"\n",
        "\n",
        "# GPUç”¨ã®æœ€é©åŒ–è¨­å®š\n",
        "BATCH_SIZE = 16  # GPUãªã‚‰å¤§ããè¨­å®šå¯èƒ½ï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
        "EPOCHS = 10\n",
        "MAX_NODES = 1000\n",
        "HIDDEN_DIM = 256\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# ãƒ‘ã‚¹ã®å†ç¢ºèªï¼ˆCell 9ã‚’å®Ÿè¡Œã—ã¦ã„ãªã„å ´åˆã«å‚™ãˆã¦ï¼‰\n",
        "try:\n",
        "    # GRAPHS_FILEã¨LABELS_FILEãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "    if 'GRAPHS_FILE' not in globals() or 'LABELS_FILE' not in globals():\n",
        "        print(\"âš ï¸ ãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Cell 9ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "        print(\"æ‰‹å‹•ã§ãƒ‘ã‚¹ã‚’è¨­å®šã™ã‚‹å ´åˆ:\")\n",
        "        print('  GRAPHS_FILE = \"/content/drive/MyDrive/ã‚ãªãŸã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/training_data/train_graphs.jsonl\"')\n",
        "        print('  LABELS_FILE = \"/content/drive/MyDrive/ã‚ãªãŸã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/training_data/train_labels.jsonl\"')\n",
        "        raise NameError(\"GRAPHS_FILE or LABELS_FILE not defined\")\n",
        "    \n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«: {GRAPHS_FILE}\")\n",
        "    print(f\"  å­˜åœ¨: {os.path.exists(GRAPHS_FILE)}\")\n",
        "    if os.path.exists(GRAPHS_FILE):\n",
        "        file_size = os.path.getsize(GRAPHS_FILE) / (1024 * 1024)\n",
        "        print(f\"  ã‚µã‚¤ã‚º: {file_size:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {LABELS_FILE}\")\n",
        "    print(f\"  å­˜åœ¨: {os.path.exists(LABELS_FILE)}\")\n",
        "    if os.path.exists(LABELS_FILE):\n",
        "        file_size = os.path.getsize(LABELS_FILE) / (1024 * 1024)\n",
        "        print(f\"  ã‚µã‚¤ã‚º: {file_size:.2f} MB\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å­¦ç¿’ã‚’å®Ÿè¡Œ\n",
        "    if os.path.exists(GRAPHS_FILE) and os.path.exists(LABELS_FILE):\n",
        "        print(\"âœ“ ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæº–å‚™ã§ãã¾ã—ãŸã€‚å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "        print()\n",
        "        \n",
        "        !python3 -m ml.train \\\n",
        "            --graphs \"{GRAPHS_FILE}\" \\\n",
        "            --labels \"{LABELS_FILE}\" \\\n",
        "            --output-dir \"{OUTPUT_DIR}\" \\\n",
        "            --batch-size {BATCH_SIZE} \\\n",
        "            --epochs {EPOCHS} \\\n",
        "            --hidden-dim {HIDDEN_DIM} \\\n",
        "            --max-nodes {MAX_NODES} \\\n",
        "            --device cuda \\\n",
        "            --gnn-type GAT \\\n",
        "            --lr {LEARNING_RATE}\n",
        "    else:\n",
        "        print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "        print(\"\\nå¯¾å‡¦æ–¹æ³•:\")\n",
        "        print(\"1. Cell 9ã‚’å®Ÿè¡Œã—ã¦ãƒ‘ã‚¹ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãã ã•ã„\")\n",
        "        print(\"2. ã¾ãŸã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ‰‹å‹•ã§ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¦ãã ã•ã„:\")\n",
        "        print()\n",
        "        print(\"   # Google Driveå†…ã®å®Ÿéš›ã®ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„\")\n",
        "        print('   GRAPHS_FILE = \"/content/drive/MyDrive/å®Ÿéš›ã®ãƒ‘ã‚¹/training_data/train_graphs.jsonl\"')\n",
        "        print('   LABELS_FILE = \"/content/drive/MyDrive/å®Ÿéš›ã®ãƒ‘ã‚¹/training_data/train_labels.jsonl\"')\n",
        "        print()\n",
        "        print(\"3. Google Driveå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã‚’ç¢ºèª:\")\n",
        "        print(\"   !ls -la /content/drive/MyDrive/\")\n",
        "        print(\"   !find /content/drive/MyDrive -name 'train_graphs.jsonl' 2>/dev/null\")\n",
        "        \n",
        "except NameError as e:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    print(\"\\nCell 9ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. çµæœã®ç¢ºèªã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’å±¥æ­´ã®å¯è¦–åŒ–\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_file = f\"{OUTPUT_DIR}/training_history.json\"\n",
        "if os.path.exists(history_file):\n",
        "    with open(history_file) as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Acc')\n",
        "    plt.plot(history['val_acc'], label='Val Acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training history file not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’Google Driveã«ä¿å­˜\n",
        "import shutil\n",
        "\n",
        "drive_checkpoint_dir = f\"{DATASET_BASE}/checkpoints\"\n",
        "os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    # æ—¢å­˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚³ãƒ”ãƒ¼\n",
        "    if os.path.exists(drive_checkpoint_dir):\n",
        "        shutil.rmtree(drive_checkpoint_dir)\n",
        "    shutil.copytree(OUTPUT_DIR, drive_checkpoint_dir)\n",
        "    print(f\"âœ“ Checkpoints saved to {drive_checkpoint_dir}\")\n",
        "else:\n",
        "    print(\"Checkpoint directory not found\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
