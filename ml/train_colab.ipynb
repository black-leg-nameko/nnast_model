{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NNAST Model Training on Google Colab\n",
        "\n",
        "This notebook trains the NNAST model using GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 環境セットアップ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU確認\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# リポジトリのクローンまたはコードのアップロード\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# 方法1: Gitリポジトリからクローン（推奨）\n",
        "# !git clone https://github.com/yourusername/nnast_model.git\n",
        "# os.chdir('nnast_model')\n",
        "\n",
        "# 方法2: zipファイルをアップロードして解凍\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.zip'):\n",
        "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#             zip_ref.extractall('.')\n",
        "#         print(f\"Extracted {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 依存関係のインストール\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-geometric\n",
        "!pip install transformers\n",
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. データセットのアップロード\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 方法1: Google Driveからマウント（推奨）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
<<<<<<< HEAD
        "# データセットをGoogle Driveにアップロードしてから、ここでパスを指定\n",
        "# 例: /content/drive/MyDrive/nnast_dataset/training_data\n",
        "DATASET_BASE = \"/content/drive/MyDrive/nnast_dataset\"\n"
=======
        "# データセットのパスを自動検出または手動指定\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# よくあるパスパターンを検索\n",
        "possible_bases = [\n",
        "    \"/content/drive/MyDrive/nnast_dataset\",\n",
        "    \"/content/drive/MyDrive/nnast_colab_dataset\",\n",
        "    \"/content/drive/MyDrive/nnast_model\",\n",
        "]\n",
        "\n",
        "# データセットファイルを探す\n",
        "found_base = None\n",
        "for base in possible_bases:\n",
        "    graphs_file = os.path.join(base, \"training_data\", \"train_graphs.jsonl\")\n",
        "    if os.path.exists(graphs_file):\n",
        "        found_base = base\n",
        "        print(f\"✓ データセットが見つかりました: {base}\")\n",
        "        break\n",
        "\n",
        "if found_base:\n",
        "    DATASET_BASE = found_base\n",
        "else:\n",
        "    # 手動でパスを指定する場合\n",
        "    print(\"⚠️ 自動検出できませんでした。手動でパスを指定してください。\")\n",
        "    print(\"\\nGoogle Drive内のデータセットディレクトリを確認:\")\n",
        "    print(\"例: /content/drive/MyDrive/nnast_colab_dataset\")\n",
        "    \n",
        "    # MyDrive内のディレクトリ一覧を表示\n",
        "    mydrive_path = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(mydrive_path):\n",
        "        print(\"\\nMyDrive内のディレクトリ:\")\n",
        "        for item in sorted(os.listdir(mydrive_path)):\n",
        "            item_path = os.path.join(mydrive_path, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                print(f\"  - {item_path}\")\n",
        "    \n",
        "    # デフォルト値を設定（ユーザーが変更可能）\n",
        "    DATASET_BASE = \"/content/drive/MyDrive/nnast_colab_dataset\"  # エラーメッセージから推測\n",
        "    print(f\"\\nデフォルトパス: {DATASET_BASE}\")\n",
        "    print(\"必要に応じて上記のDATASET_BASEを変更してください。\")\n"
>>>>>>> f2bb8e8 (update train_colab.ipynb)
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 方法2: 直接アップロード（小規模データセット用）\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# \n",
        "# import zipfile\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.zip'):\n",
        "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#             zip_ref.extractall('.')\n",
        "#         print(f\"Extracted {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
<<<<<<< HEAD
        "## 3. 学習の実行\n"
=======
        "## 3. データセットパスの確認\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データセットファイルの存在確認\n",
        "GRAPHS_FILE = os.path.join(DATASET_BASE, \"training_data\", \"train_graphs.jsonl\")\n",
        "LABELS_FILE = os.path.join(DATASET_BASE, \"training_data\", \"train_labels.jsonl\")\n",
        "\n",
        "print(f\"データセットベース: {DATASET_BASE}\")\n",
        "print(f\"グラフファイル: {GRAPHS_FILE}\")\n",
        "print(f\"ラベルファイル: {LABELS_FILE}\")\n",
        "print()\n",
        "\n",
        "# ファイル存在確認\n",
        "if os.path.exists(GRAPHS_FILE):\n",
        "    file_size = os.path.getsize(GRAPHS_FILE) / (1024 * 1024)  # MB\n",
        "    print(f\"✓ train_graphs.jsonl が見つかりました ({file_size:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"✗ train_graphs.jsonl が見つかりません: {GRAPHS_FILE}\")\n",
        "    print(\"  パスを確認してください。\")\n",
        "\n",
        "if os.path.exists(LABELS_FILE):\n",
        "    file_size = os.path.getsize(LABELS_FILE) / (1024 * 1024)  # MB\n",
        "    print(f\"✓ train_labels.jsonl が見つかりました ({file_size:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"✗ train_labels.jsonl が見つかりません: {LABELS_FILE}\")\n",
        "    print(\"  パスを確認してください。\")\n",
        "\n",
        "# 両方のファイルが存在する場合のみ学習を続行\n",
        "if os.path.exists(GRAPHS_FILE) and os.path.exists(LABELS_FILE):\n",
        "    print(\"\\n✓ すべてのファイルが準備できました。学習を開始できます。\")\n",
        "else:\n",
        "    print(\"\\n⚠️ ファイルが見つかりません。以下を確認してください:\")\n",
        "    print(\"  1. Google Driveにデータセットがアップロードされているか\")\n",
        "    print(\"  2. DATASET_BASEのパスが正しいか\")\n",
        "    print(\"  3. training_data/ディレクトリ内にtrain_graphs.jsonlとtrain_labels.jsonlがあるか\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 学習の実行\n"
>>>>>>> f2bb8e8 (update train_colab.ipynb)
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 学習パラメータ\n",
<<<<<<< HEAD
        "GRAPHS_FILE = f\"{DATASET_BASE}/training_data/train_graphs.jsonl\"  # パスを調整\n",
        "LABELS_FILE = f\"{DATASET_BASE}/training_data/train_labels.jsonl\"\n",
=======
>>>>>>> f2bb8e8 (update train_colab.ipynb)
        "OUTPUT_DIR = \"./checkpoints\"\n",
        "\n",
        "# GPU用の最適化設定\n",
        "BATCH_SIZE = 16  # GPUなら大きく設定可能（メモリに応じて調整）\n",
        "EPOCHS = 10\n",
        "MAX_NODES = 1000\n",
        "HIDDEN_DIM = 256\n",
<<<<<<< HEAD
        "LEARNING_RATE = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 学習実行\n",
        "!python3 -m ml.train \\\n",
        "    --graphs {GRAPHS_FILE} \\\n",
        "    --labels {LABELS_FILE} \\\n",
        "    --output-dir {OUTPUT_DIR} \\\n",
        "    --batch-size {BATCH_SIZE} \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --hidden-dim {HIDDEN_DIM} \\\n",
        "    --max-nodes {MAX_NODES} \\\n",
        "    --device cuda \\\n",
        "    --gnn-type GAT \\\n",
        "    --lr {LEARNING_RATE}\n"
=======
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# ファイルが存在する場合のみ学習を実行\n",
        "if os.path.exists(GRAPHS_FILE) and os.path.exists(LABELS_FILE):\n",
        "    print(\"学習を開始します...\")\n",
        "    print(f\"グラフファイル: {GRAPHS_FILE}\")\n",
        "    print(f\"ラベルファイル: {LABELS_FILE}\")\n",
        "    print(f\"出力ディレクトリ: {OUTPUT_DIR}\")\n",
        "    print()\n",
        "    \n",
        "    !python3 -m ml.train \\\n",
        "        --graphs \"{GRAPHS_FILE}\" \\\n",
        "        --labels \"{LABELS_FILE}\" \\\n",
        "        --output-dir \"{OUTPUT_DIR}\" \\\n",
        "        --batch-size {BATCH_SIZE} \\\n",
        "        --epochs {EPOCHS} \\\n",
        "        --hidden-dim {HIDDEN_DIM} \\\n",
        "        --max-nodes {MAX_NODES} \\\n",
        "        --device cuda \\\n",
        "        --gnn-type GAT \\\n",
        "        --lr {LEARNING_RATE}\n",
        "else:\n",
        "    print(\"⚠️ データセットファイルが見つかりません。上記のセルでパスを確認してください。\")\n"
>>>>>>> f2bb8e8 (update train_colab.ipynb)
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
<<<<<<< HEAD
        "## 4. 結果の確認とダウンロード\n"
=======
        "## 5. 結果の確認とダウンロード\n"
>>>>>>> f2bb8e8 (update train_colab.ipynb)
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 学習履歴の可視化\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_file = f\"{OUTPUT_DIR}/training_history.json\"\n",
        "if os.path.exists(history_file):\n",
        "    with open(history_file) as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Acc')\n",
        "    plt.plot(history['val_acc'], label='Val Acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training history file not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# チェックポイントをGoogle Driveに保存\n",
        "import shutil\n",
        "\n",
        "drive_checkpoint_dir = f\"{DATASET_BASE}/checkpoints\"\n",
        "os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    # 既存のディレクトリをコピー\n",
        "    if os.path.exists(drive_checkpoint_dir):\n",
        "        shutil.rmtree(drive_checkpoint_dir)\n",
        "    shutil.copytree(OUTPUT_DIR, drive_checkpoint_dir)\n",
        "    print(f\"✓ Checkpoints saved to {drive_checkpoint_dir}\")\n",
        "else:\n",
        "    print(\"Checkpoint directory not found\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
