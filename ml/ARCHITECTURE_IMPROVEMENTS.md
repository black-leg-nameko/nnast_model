# CPG+GNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ææ¡ˆ

## ç¾çŠ¶ã®èª²é¡Œã¨æ”¹å–„ææ¡ˆ

### ğŸ”´ 1. ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ã®æœªæ´»ç”¨ï¼ˆé‡è¦åº¦: é«˜ï¼‰

**ç¾çŠ¶**: `edge_attr`ã¯ä½œæˆã•ã‚Œã¦ã„ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„
- CPGã«ã¯AST/CFG/DFG/DDFGãªã©é‡è¦ãªã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ãŒã‚ã‚‹
- ã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—æƒ…å ±ã¯è„†å¼±æ€§æ¤œå‡ºã«é‡è¦ï¼ˆä¾‹: DFGã‚¨ãƒƒã‚¸ã¯ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ã«é‡è¦ï¼‰

**æ”¹å–„æ¡ˆ**:
```python
# Edge-aware GNN layer
from torch_geometric.nn import GATConv, GCNConv
from torch_geometric.nn import TransformerConv  # Edge-aware attention

# Option 1: Edge-type embedding
self.edge_embedding = nn.Embedding(num_edge_types, edge_dim)

# Option 2: Edge-type specific GNN layers
self.ast_layer = GATConv(hidden_dim, hidden_dim)
self.cfg_layer = GATConv(hidden_dim, hidden_dim)
self.dfg_layer = GATConv(hidden_dim, hidden_dim)
# ã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ã”ã¨ã«ç•°ãªã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‡¦ç†ã—ã€å¾Œã§çµ±åˆ
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Edge-type aware vulnerability detection" - CPGã®å¤šæ§˜ãªã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ã‚’æ´»ç”¨

---

### ğŸ”´ 2. ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®æœªæ´»ç”¨ï¼ˆé‡è¦åº¦: é«˜ï¼‰

**ç¾çŠ¶**: `node_kinds`, `type_hint`, `symbol`ãªã©ã®æƒ…å ±ãŒæ´»ç”¨ã•ã‚Œã¦ã„ãªã„
- CodeBERTã®ã¿ã«ä¾å­˜ã—ã¦ã„ã‚‹ãŒã€æ§‹é€ çš„æƒ…å ±ã‚‚é‡è¦

**æ”¹å–„æ¡ˆ**:
```python
# Multi-modal node features
node_kind_emb = nn.Embedding(num_node_kinds, kind_dim)
type_hint_emb = nn.Embedding(num_types, type_dim)

# Combine: CodeBERT + structural features
x_combined = torch.cat([
    codebert_emb,           # (768-dim)
    node_kind_emb(node_kinds),  # (kind_dim)
    type_hint_emb(type_hints), # (type_dim)
], dim=1)
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Hybrid semantic-structural node representation"

---

### ğŸŸ¡ 3. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´æŠ½å‡ºã®ä¸è¶³ï¼ˆé‡è¦åº¦: ä¸­-é«˜ï¼‰

**ç¾çŠ¶**: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ã‚’çµ±åˆã—ã¦ã„ãªã„
- æµ…ã„å±¤: å±€æ‰€çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
- æ·±ã„å±¤: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªä¾å­˜é–¢ä¿‚

**æ”¹å–„æ¡ˆ**:
```python
# Multi-scale feature aggregation
layer_outputs = []
for layer in self.gnn_layers:
    x = layer(x, edge_index)
    layer_outputs.append(x)

# Hierarchical pooling
x_multi_scale = torch.cat([
    global_mean_pool(layer_outputs[0], batch),  # Local
    global_mean_pool(layer_outputs[-1], batch), # Global
    global_max_pool(layer_outputs[-1], batch),
], dim=1)
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Multi-scale vulnerability pattern detection"

---

### ğŸŸ¡ 4. éšå±¤çš„è¡¨ç¾å­¦ç¿’ã®ä¸è¶³ï¼ˆé‡è¦åº¦: ä¸­ï¼‰

**ç¾çŠ¶**: é–¢æ•°ãƒ¬ãƒ™ãƒ«ã®ã¿ã§ã€ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ã®æƒ…å ±ãŒãªã„
- è„†å¼±æ€§ã¯é–¢æ•°é–“ã®ç›¸äº’ä½œç”¨ã§ç™ºç”Ÿã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹

**æ”¹å–„æ¡ˆ**:
```python
# Hierarchical GNN
class HierarchicalCPGModel(nn.Module):
    def __init__(self):
        # Function-level GNN
        self.function_gnn = CPGTaintFlowModel(...)
        
        # File-level aggregation
        self.file_gnn = GATConv(...)
        
        # Project-level aggregation
        self.project_gnn = GATConv(...)
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Hierarchical code representation for vulnerability detection"

---

### ğŸŸ¡ 5. ä½ç½®æƒ…å ±ã®æœªæ´»ç”¨ï¼ˆé‡è¦åº¦: ä¸­ï¼‰

**ç¾çŠ¶**: `spans`æƒ…å ±ãŒã‚ã‚‹ãŒæ´»ç”¨ã•ã‚Œã¦ã„ãªã„
- ã‚³ãƒ¼ãƒ‰ã®ä½ç½®æƒ…å ±ã¯è„†å¼±æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ç›¸é–¢ãŒã‚ã‚‹

**æ”¹å–„æ¡ˆ**:
```python
# Positional encoding for code spans
class CodePositionalEncoding(nn.Module):
    def __init__(self, d_model):
        self.line_emb = nn.Embedding(max_lines, d_model // 2)
        self.col_emb = nn.Embedding(max_cols, d_model // 2)
    
    def forward(self, spans):
        line_emb = self.line_emb(spans[:, 0])
        col_emb = self.col_emb(spans[:, 1])
        return torch.cat([line_emb, col_emb], dim=1)
```

---

### ğŸŸ¢ 6. è§£é‡ˆå¯èƒ½æ€§ã®ä¸è¶³ï¼ˆé‡è¦åº¦: ä¸­-é«˜ï¼‰

**ç¾çŠ¶**: ã©ã®ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸ãŒé‡è¦ã‹ã‚’ç¤ºã™æ©Ÿæ§‹ãŒãªã„
- å®Ÿç”¨åŒ–ã«ã¯ã€Œãªãœè„†å¼±ã¨åˆ¤æ–­ã—ãŸã‹ã€ã®èª¬æ˜ãŒå¿…è¦

**æ”¹å–„æ¡ˆ**:
```python
# Attention weights for interpretability
class InterpretableGAT(nn.Module):
    def forward(self, x, edge_index):
        x, attention_weights = self.gat(x, edge_index, return_attention_weights=True)
        return x, attention_weights

# Node importance scoring
node_importance = attention_weights.mean(dim=1)
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Explainable vulnerability detection with attention visualization"

---

### ğŸŸ¢ 7. å¤§ããªã‚°ãƒ©ãƒ•ã¸ã®å¯¾å¿œä¸è¶³ï¼ˆé‡è¦åº¦: é«˜ãƒ»å®Ÿç”¨æ€§ï¼‰

**ç¾çŠ¶**: `max_nodes`ã§åˆ‡ã‚Šè©°ã‚ã¦ã„ã‚‹
- å®Ÿã‚³ãƒ¼ãƒ‰ã§ã¯æ•°åƒãƒãƒ¼ãƒ‰ã®ã‚°ãƒ©ãƒ•ãŒä¸€èˆ¬çš„

**æ”¹å–„æ¡ˆ**:
```python
# Graph sampling strategies
from torch_geometric.loader import NeighborSampler

# Option 1: Subgraph sampling
# Option 2: Hierarchical pooling (å…ˆã«å°ã•ãªã‚µãƒ–ã‚°ãƒ©ãƒ•ã«åˆ†å‰²)
# Option 3: Graph Transformer with efficient attention
from torch_geometric.nn import TransformerConv

# Option 4: Graph coarsening
class GraphCoarsening(nn.Module):
    """å¤§ããªã‚°ãƒ©ãƒ•ã‚’å°ã•ãªã‚°ãƒ©ãƒ•ã«ç²—åŒ–"""
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Scalable vulnerability detection for large codebases"

---

### ğŸŸ¢ 8. è»¢ç§»å­¦ç¿’ãƒ»äº‹å‰å­¦ç¿’ã®ä¸è¶³ï¼ˆé‡è¦åº¦: é«˜ï¼‰

**ç¾çŠ¶**: ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’
- ã‚³ãƒ¼ãƒ‰è¡¨ç¾ã®äº‹å‰å­¦ç¿’ãŒæœ‰åŠ¹

**æ”¹å–„æ¡ˆ**:
```python
# Self-supervised pre-training tasks
class PreTrainingTasks:
    # Task 1: Masked code prediction
    # Task 2: Edge prediction (AST/CFG/DFG)
    # Task 3: Node type prediction
    # Task 4: Graph contrastive learning

# Pre-train on large code corpus
# Fine-tune on vulnerability detection
```

**è«–æ–‡ã§ã®è²¢çŒ®**: "Self-supervised pre-training for code vulnerability detection"

---

### ğŸŸ¢ 9. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®ä¸è¶³ï¼ˆé‡è¦åº¦: ä¸­ï¼‰

**ç¾çŠ¶**: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãŒãªã„
- ã‚³ãƒ¼ãƒ‰ã®å¤šæ§˜æ€§ã‚’å¢—ã‚„ã™

**æ”¹å–„æ¡ˆ**:
```python
# Code augmentation strategies
class CodeAugmentation:
    # 1. Variable renaming (semantic preserving)
    # 2. Dead code insertion
    # 3. Control flow restructuring (equivalent)
    # 4. Graph-level augmentation (edge dropping, node masking)
```

---

### ğŸŸ¢ 10. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã®æ”¹å–„ï¼ˆé‡è¦åº¦: ä¸­ï¼‰

**ç¾çŠ¶**: æ¨™æº–çš„ãªãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
- ã‚³ãƒ¼ãƒ‰ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

**æ”¹å–„æ¡ˆ**:
```python
# Code-specific attention mechanisms
class CodeAwareAttention(nn.Module):
    """ã‚³ãƒ¼ãƒ‰ã®æ§‹é€ ã‚’è€ƒæ…®ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³"""
    # 1. Distance-aware attention (è¿‘ã„ãƒãƒ¼ãƒ‰ã‚’é‡è¦–)
    # 2. Edge-type aware attention (DFGã‚¨ãƒƒã‚¸ã‚’é‡è¦–)
    # 3. Hierarchical attention (é–¢æ•°å†…/é–¢æ•°é–“)
```

---

## å„ªå…ˆåº¦åˆ¥å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: å³åº§ã«å®Ÿè£…ã™ã¹ãï¼ˆç²¾åº¦å‘ä¸Šã«ç›´çµï¼‰
1. âœ… **ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ã®æ´»ç”¨** - Edge-type aware GNN
2. âœ… **ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®çµ±åˆ** - Multi-modal node features
3. âœ… **ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´æŠ½å‡º** - Layer-wise aggregation

### Phase 2: è«–æ–‡ã®æ–°è¦æ€§å‘ä¸Š
4. âœ… **éšå±¤çš„è¡¨ç¾å­¦ç¿’** - Function/File/Project levels
5. âœ… **è§£é‡ˆå¯èƒ½æ€§** - Attention visualization
6. âœ… **è»¢ç§»å­¦ç¿’** - Self-supervised pre-training

### Phase 3: å®Ÿç”¨åŒ–
7. âœ… **å¤§ããªã‚°ãƒ©ãƒ•å¯¾å¿œ** - Graph sampling/coarsening
8. âœ… **æ¨è«–é€Ÿåº¦æœ€é©åŒ–** - Model compression, quantization
9. âœ… **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ** - Code augmentation

---

## è«–æ–‡ã§ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ

### 1. **CPGã®å¤šæ§˜ãªã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ã®æ´»ç”¨**
- AST/CFG/DFG/DDFGã‚’åŒºåˆ¥ã—ã¦å‡¦ç†
- ã‚¨ãƒƒã‚¸ã‚¿ã‚¤ãƒ—ã”ã¨ã®é‡è¦åº¦å­¦ç¿’

### 2. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆ**
- CodeBERTï¼ˆæ„å‘³ï¼‰ + æ§‹é€ ç‰¹å¾´ï¼ˆãƒãƒ¼ãƒ‰ç¨®é¡ã€å‹æƒ…å ±ï¼‰
- 3æ®µéšèåˆï¼ˆEarly/Intermediate/Lateï¼‰

### 3. **éšå±¤çš„è„†å¼±æ€§æ¤œå‡º**
- é–¢æ•°ãƒ¬ãƒ™ãƒ« â†’ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ« â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«
- é–¢æ•°é–“ã®ç›¸äº’ä½œç”¨ã‚’è€ƒæ…®

### 4. **è§£é‡ˆå¯èƒ½ãªæ¤œå‡º**
- ã©ã®ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸ãŒè„†å¼±æ€§ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹å¯è¦–åŒ–
- é–‹ç™ºè€…ã¸ã®èª¬æ˜å¯èƒ½æ€§

### 5. **å®Ÿç”¨çš„ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**
- å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã¸ã®å¯¾å¿œ
- åŠ¹ç‡çš„ãªæ¨è«–

---

## ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒã®ææ¡ˆ

### æ¯”è¼ƒã™ã¹ãæ‰‹æ³•:
1. **Rule-based**: Bandit, SonarQube
2. **Classical ML**: Random Forest (token features)
3. **Sequence models**: Transformer encoder (code as sequence)
4. **Graph models**: 
   - Standard GCN/GAT (baseline)
   - CodeBERT + GNN (ablation)
   - ææ¡ˆæ‰‹æ³•ï¼ˆfull modelï¼‰

### è©•ä¾¡æŒ‡æ¨™:
- Top-K Recall (K=5, 10, 20)
- Precision@K
- AUROC, AUPRC
- **æ–°è¦**: False Positive Rate@K (å®Ÿç”¨æ€§)
- **æ–°è¦**: Inference time (å®Ÿç”¨æ€§)

---

## å®Ÿè£…ã®å„ªå…ˆé †ä½

1. **æœ€å„ªå…ˆ**: ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡æ´»ç”¨ï¼ˆå®Ÿè£…å®¹æ˜“ã€åŠ¹æœå¤§ï¼‰
2. **é«˜å„ªå…ˆ**: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡çµ±åˆã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«
3. **ä¸­å„ªå…ˆ**: éšå±¤çš„è¡¨ç¾ã€è§£é‡ˆå¯èƒ½æ€§
4. **ä½å„ªå…ˆ**: è»¢ç§»å­¦ç¿’ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ãŒåŠ¹æœå¤§ï¼‰
